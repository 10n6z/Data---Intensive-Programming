{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f4a1bec-0ec5-4c7e-8611-9790d1a3c79a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Basic instructions to Databricks Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8bf8df2-59b6-4387-8e46-7db1d3f11bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Databricks notebooks are an easy way to execute Apache Spark in cloud environments with browser.\n",
    "Here are a few pointers on how to get started.\n",
    "\n",
    "For more information, see [official instructions](https://learn.microsoft.com/en-gb/azure/databricks/notebooks/notebooks-code).\n",
    "\n",
    "### Going around in the workspace\n",
    "* Your code is, on default, stored in the Databricks Workspace path. You can get there by selecting \"Workspace\" in the left menu and then Workspace / Users / Your email.\n",
    "* The workspace will be destroyed in the end of the course. Thus, all your code will disappear.\n",
    "* The recommended way is to use Databricks git repository support. The instructions for this are shown on the first weekly assignment and in Moodle. You see the repositories by selecting \"Workspace\" in the left menu and then Repos / Your email.\n",
    "\n",
    "### Executing code\n",
    "* To do anything useful, you need to attach your notebook to a computational cluster. You can do this behind the top right \"Connect\" button in the notebook.\n",
    "* If nobody else is using the cluster, the cluster might start for you. This might take ~5 minutes.\n",
    "* All the students are using the same computational cluster. It scales up and down automatically depending on the usage. Scaling might also take ~5 minutes to take effect.\n",
    "* Notebooks are divided to cells. You can execute cell code by clicking top right corner play button or by using \"Shift\"+\"Enter\" shortcut.\n",
    "* You can see the other keyboard shortcuts via Help / Keyboard shortcuts\n",
    "* As you hopefully remember, Apache Spark cluster consists of Driver and Worker nodes. All Spark commands are shared between workers, all Scala and Python code is executed on Driver node. Thus, do not execute heavy load without Spark.\n",
    "\n",
    "### Selecting programming language\n",
    "* Each notebook has a default language. You can select it on the top of the notebook, right side of the notebook name. In this notebook, the default language is \"Scala\"\n",
    "* Cells can have different languages. However, you can not directly refer to variables in other languages.\n",
    "* You can select the cell wise language from the top right corner of the cell. Alternatives are Scala, Python, and Markdown (for documentation). Databricks also supports R and SQL cells, but they are not allowed in course assignments.\n",
    "* You can also use \"magic commands\" on the first line of cell to set the language. The magic commands are `%scala`, `%python`and `%md`.\n",
    "\n",
    "### Working with cells\n",
    "* You can add and remove cells, and change their order.\n",
    "* You can also execute cells in any order.\n",
    "* This means, you will probably end up in very confusing ordering of cells. And notebooks that do not actually work when executing from the beginning to end.\n",
    "* That is, your assignment might not work for us if you are not careful. Remember to try execution in the correct order.\n",
    "* Using functional programming practices helps in this.\n",
    "\n",
    "### Important\n",
    "* Before submitting your code, **always execute notebook fully with clean state**. This is done by selecting Run / Clear state and run all.\n",
    "* Due to permission restrictions, every student can restart the cluster. **Do NOT restart clusters**. Otherwise, everyone's notebook executions will be cleared. Codes will not disappear.\n",
    "* Avoid executing heavy loads on Spark Driver node. Instead, **distribute the computation with Spark framework**: in practice, use commands that start with `spark.` or `dataframe.`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "834b497e-8633-447b-9f31-d0f58096cac6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Let's check some practical code examples**\n",
    "\n",
    "This notebook is stored in the common folder `/shared_readonly/` so everyone is working on it at the same time.\n",
    "You probably want to take a copy of it to your own folder. You can do it via File / Clone on the top menu of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e29721-a83f-4156-9609-0476055d8fca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Let us execute Scala\n",
    "val string_list : List[String] = List(\"This\", \"is\", \"Scala\")\n",
    "val printed_string = string_list.mkString(\" \")\n",
    "println(\"** Let's start our first printing.\")\n",
    "println(printed_string + \" code!\")\n",
    "println(\"** Successfully printed our first text. Rest is other cell output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca9c4c5-959f-49d7-9e0c-1013fedf8200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Let's execute Python. Note the magic command above\n",
    "string_list = [\"This\", \"is\", \"Python\"]\n",
    "printed_string = \" \".join(string_list)\n",
    "print(printed_string + \" code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1e2ad2-115c-4873-92b0-8991f396dd93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Then some example Spark code.**\n",
    "\n",
    "This Markdown cell is created with magic command. Double click this cell to see the magic command `%md` on the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1dce286-4847-4bc5-b5e2-8c6ac2b186c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Let us work on Scala and Apache Spark\n",
    "\n",
    "// Create a sample DataFrame\n",
    "val data = Seq(\n",
    "  (\"Alice\", 25),\n",
    "  (\"Bob\", 30),\n",
    "  (\"Charlie\", 35)\n",
    ")\n",
    "\n",
    "// Let us do our first Apache Spark call. You can recognize it on the next line because we use library \"spark\"\n",
    "val df = spark.createDataFrame(data).toDF(\"Name\", \"Age\")\n",
    "// Above code is now be divided between worker nodes\n",
    "\n",
    "// Use Apache Spark print to show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e93895-9e5c-4c87-a1e2-74d6bec50700",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// However, Databricks also has a way to print DataFrames in nicer format\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d7eb01-076b-44d1-acde-ff78b5483d99",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Let us do a bit of computation with Spark\n",
    "import org.apache.spark.sql.functions._ // Very often DataFrame computations need this import\n",
    "\n",
    "val avgAge = df.select(avg(\"Age\")).head().getDouble(0)\n",
    "val maxAge = df.select(max(\"Age\")).head().getInt(0)\n",
    "println(s\"Average Age: $avgAge\")\n",
    "println(s\"Maximum Age: $maxAge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "affbf878-05d1-4dce-a0f6-85a182ed2cf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reading files\n",
    "\n",
    "The environment is in the cloud. So, the accessed data is also stored in the cloud. In this case, we use Azure Storage Account and Azure Data Lake Storage Gen2. You can get there with your Microsoft account by installing [Microsoft Azure Storage Explorer](https://azure.microsoft.com/en-us/products/storage/storage-explorer) to your machine or with browser on following URLs:\n",
    "* [Shared container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2024-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2024gen2/path/shared/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) for example data sets.\n",
    "* [Student container](https://portal.azure.com/#view/Microsoft_Azure_Storage/ContainerMenuBlade/~/overview/storageAccountId/%2Fsubscriptions%2Fe0c78478-e7f8-429c-a25f-015eae9f54bb%2FresourceGroups%2Ftuni-cs320-f2024-rg%2Fproviders%2FMicrosoft.Storage%2FstorageAccounts%2Ftunics320f2024gen2/path/students/etag/%220x8DBB0695B02FFFE%22/defaultEncryptionScope/%24account-encryption-key/denyEncryptionScopeOverride~/false/defaultId//publicAccessVal/None) where you can store your own data. Please create a folder for yourself with your name.\n",
    "* You also find all the relevant Azure resources by going to the [Azure portal](https://portal.azure.com) to Storage accounts and select [`tunics320f2024gen2` /  containers](https://portal.azure.com/#@tuni.onmicrosoft.com/resource/subscriptions/e0c78478-e7f8-429c-a25f-015eae9f54bb/resourceGroups/tuni-cs320-f2024-rg/providers/Microsoft.Storage/storageAccounts/tunics320f2024gen2/containersList).\n",
    "\n",
    "Now, let's go through some examples on how to read the data int the storage with Databricks Apache Spark. The address for the data is\n",
    "`abfss://<container>@tunics320f2024gen2.dfs.core.windows.net/<path>/<to>/<file.csv>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9271d2bd-d138-4728-8ad0-c4b4d83126e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val file_csv = \"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/demo/kaggle/csv/10mb_imdb_anime.csv\"\n",
    "val df_csv = spark.read\n",
    "  .option(\"header\", \"true\")       // The first row has column names\n",
    "  .option(\"sep\", \",\")             // \",\" is used as the column separator in the CSV file\n",
    "  .option(\"inferSchema\", \"true\")  // Try to automatically determine the data types for the columns\n",
    "  .csv(file_csv)\n",
    "\n",
    "display(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26955d09-eb49-4e07-b7c7-f470df10a7f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# The same with Python\n",
    "file = \"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/demo/kaggle/csv/10mb_imdb_anime.csv\"\n",
    "df = spark.read  \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .csv(file)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fcd3667-721c-4be5-88d0-6aa0877dd647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Typically we use some more suitable file format, like Parquet.\n",
    "// Column format is stored in the file itself, so we do not need to give it.\n",
    "val file_parquet = \"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/demo/kaggle/parquet/10mb_imdb_anime.parquet\"\n",
    "val df_parquet = spark.read.parquet(file_parquet)\n",
    "display(df_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd4a203f-c8a4-47b8-8af2-a6d9f81abf35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// There is alsoa a possibility to use a more advanced Delta file format, also called \"Delta lake table\"\n",
    "// We will cover this later in the course\n",
    "val file_delta = \"abfss://shared@tunics320f2024gen2.dfs.core.windows.net/demo/kaggle/delta/10mb_imdb_anime_delta\"\n",
    "val df_delta = spark.read.format(\"delta\").load(file_delta)\n",
    "display(df_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96f383c3-62d0-48c4-9b8c-d0b5e5ed6031",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Writing files\n",
    "\n",
    "Reading files is enough for this course. However, if you want, you can also try writing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c947e64-c960-422b-adf5-b0a1007fefcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val student_name = \"example_student\" // Change this to your own name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07591eb8-26dd-47c8-bec6-fa625787ec0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val target_path = s\"abfss://students@tunics320f2024gen2.dfs.core.windows.net/${student_name}/demo/\"\n",
    "val target_file = target_path + \"10mb_imdb_anime.parquet\"\n",
    "\n",
    "// Write to Parquet\n",
    "df_parquet.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(target_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d46a73b-7ece-4418-aaf2-7fc86218f758",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Use Databricks specific utilities library to see the files. Spark has Hadoop filesystem library for the same, but it is not as simple\n",
    "display(dbutils.fs.ls(target_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a38192a-4840-4b59-91e0-8e3868391bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Actually Parquet file is usually a folder with multiple files.\n",
    "// Workers might write to multiple files at the same time. Let's check the actual files\n",
    "display(dbutils.fs.ls(target_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc53de3a-5675-4039-a6b3-898c4b3eeee1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Basics of using Databricks notebooks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
